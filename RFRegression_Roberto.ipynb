{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandrePardelinha/Computer-Science-Masters/blob/main/RFRegression_Roberto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNPR3u97cVCr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JVksBU9cVCw"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "c0NTPXNlrkWL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inXvy9kBcVCx"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importando as funções loadGeneralSettings, loadDatasetColumns do arquivo 'settings' e as funções selectDatasetColumns, add_pca, preprare_training, add_regioes do arquivo 'dataset'. Ambos os códigos dos arquivos 'settings' e 'dataset' estão na pasta 'utils'.\n",
        "Deu erro (não reconhecendo o arquivo do diretório 'utils') e fiz de maneira diferente do original.** "
      ],
      "metadata": {
        "id": "nhfQiQ3FrwmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGrUcYrO77Jw",
        "outputId": "e283cd11-f2cd-4689-c782-5cfd26cd5506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "caminho_= '/content/drive/My Drive/SBDE-main'"
      ],
      "metadata": {
        "id": "QrkU6O6T62DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/My Drive/SBDE-main/utils')"
      ],
      "metadata": {
        "id": "8Wgpistn8bvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from settings import loadGeneralSettings, loadDatasetColumns\n",
        "from dataset import selectDatasetColumns, add_pca, preprare_training, add_regioes"
      ],
      "metadata": {
        "id": "MSGF7J8zANia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZHTLtzDcVCx"
      },
      "outputs": [],
      "source": [
        "#from utils.settings import loadGeneralSettings, loadDatasetColumns\n",
        "#from utils.dataset import selectDatasetColumns, add_pca, preprare_training, add_regioes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyBsHp_QcVCy"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*****************************************************************************************************"
      ],
      "metadata": {
        "id": "kXeVk4zfnwJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Esse código está dentro da pasta 'utils' com nome do arquivo 'settings'. Mesmo ajustando o 'path' corretamente, não consegui importar o arquivo da pasta 'settings' (dando erro: No such file or diretory: 'settings.yaml'). Colei aqui o código e não deu erro. O arquivo 'settings.yaml' consta o caminho para a planilha do excel 'dataset'. \n",
        "dataset:\n",
        "  path: /content/drive/My Drive/SBDE-main/data/dataset.xlsx**"
      ],
      "metadata": {
        "id": "cJl4CUHrp4Q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "def loadSettings(arquivo):\n",
        "    with open(f'/content/drive/My Drive/SBDE-main/settings/{arquivo}.yaml') as file:\n",
        "        return yaml.load(file, Loader=yaml.FullLoader)\n",
        "\n",
        "def loadGeneralSettings():\n",
        "    return loadSettings('settings')\n",
        "\n",
        "def loadDatasetColumns():\n",
        "    return loadSettings('datasetColumns')"
      ],
      "metadata": {
        "id": "NCuGMEV1Z_zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***************************************************************************************************************"
      ],
      "metadata": {
        "id": "-UPgIHbZn4Rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Esse código está dentro da pasta 'utils' com nome do arquivo 'dataset'. Mesmo ajustando o 'path' corretamente, não consegui importar os aquivos da pasta 'models', não reconhecendo os arquivos (os arquivos com formato '.sav'). Colei aqui o código e não deu erro.**"
      ],
      "metadata": {
        "id": "aW4LAuozqS2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Link do IBGE que separa em regiões: [{\"id\":1,\"sigla\":\"N\",\"nome\":\"Norte\"},{\"id\":2,\"sigla\":\"NE\",\"nome\":\"Nordeste\"},{\"id\":3,\"sigla\":\"SE\",\"nome\":\"Sudeste\"},{\"id\":4,\"sigla\":\"S\",\"nome\":\"Sul\"},{\"id\":5,\"sigla\":\"CO\",\"nome\":\"Centro-Oeste\"}]**"
      ],
      "metadata": {
        "id": "g_Rv_HlD37WD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def data_preprare(df_input, without_sr:bool=False):\n",
        "    km = pickle.load(open('/content/drive/My Drive/SBDE-main/models/kmeansclusterization.sav', 'rb'))\n",
        "    ec = pickle.load(open('/content/drive/My Drive/SBDE-main/models/encode_regiao.sav', 'rb'))\n",
        "\n",
        "    df = df_input.drop(['pubtitle'], axis=1)\n",
        "\n",
        "    if without_sr:\n",
        "        df = remove_sr(df)\n",
        "\n",
        "    df_reg = pd.DataFrame(ec.transform(df.regiao.values.reshape(-1,1)), columns=['centrooeste', 'nordeste', 'norte', 'sudeste', 'sul'])\n",
        "\n",
        "    xpred = km.predict(df.drop(['ds', 'regiao'], axis=1))\n",
        "\n",
        "    clusters = pd.DataFrame(xpred, columns=['cluster'])\n",
        "\n",
        "    df = pd.concat([df, clusters, df_reg], axis=1).drop(['regiao'], axis=1)\n",
        "\n",
        "    return (df.drop(['ds'], axis=1), df.ds)\n",
        "\n",
        "def remove_sr(df_input):\n",
        "    df =  df_input.drop(index=list(df_input[df_input.regiao=='S/R'].index))\n",
        "    df = df.reset_index()\n",
        "    return df.drop(['index'], axis=1)\n",
        "\n",
        "\n",
        "def selectDatasetColumns(df, columns):\n",
        "    columns_remove = df.columns.to_list()\n",
        "    for i in df.columns.to_list():\n",
        "        if i in columns:\n",
        "            columns_remove.remove(i)\n",
        "    return df.drop(columns=columns_remove)\n",
        "\n",
        "\n",
        "def add_pca(datasetColumns)->dict:\n",
        "    pca = pickle.load(open('/content/drive/My Drive/SBDE-main/models/pca.sav', 'rb'))\n",
        "    for i in range(0, len(pca)):\n",
        "        datasetColumns[pca[i][0]]['n_components_'+str(pca[i][1])] = i\n",
        "\n",
        "    return datasetColumns\n",
        "\n",
        "\n",
        "def add_regioes(datasetColumns)->dict:\n",
        "\n",
        "    reg = pd.read_json('https://servicodados.ibge.gov.br/api/v1/localidades/regioes')\n",
        "\n",
        "    for nome in reg.nome.to_list():\n",
        "        datasetColumns[nome] = {'all':['ds', 'sand', 'silt', 'clay', 'phh2o', 'sb', 'al', 'h', 'corg']}\n",
        "        \n",
        "    return datasetColumns\n",
        "\n",
        "def preprare_training(datasetColumns, kcluster, key, dfs_training, dfs_test, df_pub_regioes):\n",
        "\n",
        "    km = pickle.load(open('/content/drive/My Drive/SBDE-main/models/kmeansclusterization.sav', 'rb'))\n",
        "\n",
        "    pca = pickle.load(open('/content/drive/My Drive/SBDE-main/models/pca.sav', 'rb'))\n",
        "\n",
        "    reg = pd.read_json('https://servicodados.ibge.gov.br/api/v1/localidades/regioes')\n",
        "\n",
        "    if kcluster in reg.nome.to_list():\n",
        "        dfs_training = pd.merge(dfs_training, df_pub_regioes, how='left', left_on=\"pubtitle\", right_on=\"pub\")\n",
        "        dfs_training = dfs_training[dfs_training['regiao'] == str.upper(kcluster)]\n",
        "        dfs_test = pd.merge(dfs_test, df_pub_regioes, how='left', left_on=\"pubtitle\", right_on=\"pub\")\n",
        "        dfs_test = dfs_test[dfs_test['regiao'] == str.upper(kcluster)]\n",
        "\n",
        "    if 'n_components' in key:\n",
        "        columns = datasetColumns[kcluster][\"all\"]\n",
        "    else:\n",
        "        columns = datasetColumns[kcluster][key]\n",
        "\n",
        "    dfs_training_adj = selectDatasetColumns(dfs_training, columns)\n",
        "    dfs_training_x = dfs_training_adj.drop(columns=['ds'])\n",
        "    dfs_training_y = dfs_training_adj.ds\n",
        "\n",
        "    dfs_test_adj = selectDatasetColumns(dfs_test, columns)\n",
        "    dfs_test_x = dfs_test_adj.drop(columns=['ds'])\n",
        "    dfs_test_y = dfs_test_adj.ds\n",
        "\n",
        "    if 'n_components' in key:\n",
        "        dfs_training_x = pd.DataFrame(pca[datasetColumns[kcluster][key]][3].transform(dfs_training_x))\n",
        "        dfs_test_x = pd.DataFrame(pca[datasetColumns[kcluster][key]][3].transform(dfs_test_x))\n",
        "\n",
        "    elif 'cluster' in kcluster:\n",
        "\n",
        "        idx = int(kcluster.replace('cluster', ''))\n",
        "\n",
        "        X = selectDatasetColumns(dfs_training, datasetColumns['all']['all']).drop(columns=['ds'])\n",
        "        xpred = km.predict(X)\n",
        "        clusters = pd.DataFrame(xpred, columns=['cluster'])\n",
        "\n",
        "        dfs_training_x_cluster = pd.concat([dfs_training_x, clusters], axis=1)\n",
        "        dfs_training_x_cluster_idx = dfs_training_x_cluster[dfs_training_x_cluster.cluster == idx].index.tolist()\n",
        "        dfs_training_x = dfs_training_x.loc[dfs_training_x_cluster_idx]\n",
        "        dfs_training_y = dfs_training_y.loc[dfs_training_x_cluster_idx]\n",
        "\n",
        "        X = selectDatasetColumns(dfs_test, datasetColumns['all']['all']).drop(columns=['ds'])\n",
        "       \n",
        "        xpred = km.predict(X)\n",
        "        clusters = pd.DataFrame(xpred, columns=['cluster'])\n",
        "\n",
        "        dfs_test_x_cluster = pd.concat([dfs_test_x, clusters], axis=1)\n",
        "        dfs_test_x_cluster_idx = dfs_test_x_cluster[dfs_test_x_cluster.cluster == idx].index.tolist()\n",
        "        dfs_test_x = dfs_test_x.loc[dfs_test_x_cluster_idx]\n",
        "        dfs_test_y = dfs_test_y.loc[dfs_test_x_cluster_idx]\n",
        "\n",
        "    return (dfs_training_x, dfs_training_y, dfs_test_x, dfs_test_y)"
      ],
      "metadata": {
        "id": "mUaAB1aCn7DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**************************************************************************************************************************"
      ],
      "metadata": {
        "id": "wmk61Kzrn9VA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nessa fase que estava dando o erro: 'No such file or directory: 'models/pca.sav', mesmo ajustando os 'path' do arquivo 'dataset' da pasta 'utils' (o código anterior a esse). Depois que colei diretamente aqui, não deu mais esse erro.**"
      ],
      "metadata": {
        "id": "Zqp66ww310pX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwVYvPzIcVCy"
      },
      "outputs": [],
      "source": [
        "settings = loadGeneralSettings()\n",
        "datasetColumns = loadDatasetColumns()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasetColumns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dsNjTJd5l5x",
        "outputId": "5619f926-102f-4901-cea4-a2e8072f8447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'all': {'all': ['ds',\n",
              "   'sand',\n",
              "   'silt',\n",
              "   'clay',\n",
              "   'phh2o',\n",
              "   'sb',\n",
              "   'al',\n",
              "   'h',\n",
              "   'corg'],\n",
              "  'colinear1': ['ds', 'silt', 'clay', 'phh2o', 'sb', 'al', 'h', 'corg'],\n",
              "  'colinear2': ['ds', 'sand', 'silt', 'phh2o', 'sb', 'al', 'h', 'corg'],\n",
              "  'corrleq10': ['ds', 'sand', 'silt', 'clay', 'phh2o', 'h', 'corg'],\n",
              "  'corrleq20': ['ds', 'sand', 'clay', 'h', 'corg']},\n",
              " 'cluster0': {'all': ['ds',\n",
              "   'sand',\n",
              "   'silt',\n",
              "   'clay',\n",
              "   'phh2o',\n",
              "   'sb',\n",
              "   'al',\n",
              "   'h',\n",
              "   'corg'],\n",
              "  'corrleq10': ['ds', 'sand', 'clay', 'phh2o', 'sb', 'al', 'h', 'corg'],\n",
              "  'corrleq20': ['ds', 'clay', 'sb', 'h', 'corg']},\n",
              " 'cluster1': {'all': ['ds',\n",
              "   'sand',\n",
              "   'silt',\n",
              "   'clay',\n",
              "   'phh2o',\n",
              "   'sb',\n",
              "   'al',\n",
              "   'h',\n",
              "   'corg'],\n",
              "  'corrleq10': ['ds', 'sand', 'silt', 'clay', 'phh2o', 'sb', 'h', 'corg'],\n",
              "  'corrleq20': ['ds', 'silt', 'phh2o', 'h', 'corg']},\n",
              " 'cluster2': {'all': ['ds',\n",
              "   'sand',\n",
              "   'silt',\n",
              "   'clay',\n",
              "   'phh2o',\n",
              "   'sb',\n",
              "   'al',\n",
              "   'h',\n",
              "   'corg'],\n",
              "  'corrleq10': ['ds', 'clay', 'phh2o', 'al', 'h', 'corg'],\n",
              "  'corrleq20': ['ds', 'h', 'corg']},\n",
              " 'cluster3': {'all': ['ds',\n",
              "   'sand',\n",
              "   'silt',\n",
              "   'clay',\n",
              "   'phh2o',\n",
              "   'sb',\n",
              "   'al',\n",
              "   'h',\n",
              "   'corg'],\n",
              "  'corrleq10': ['ds', 'sand', 'silt', 'clay', 'sb', 'al', 'h', 'corg'],\n",
              "  'corrleq20': ['ds', 'clay', 'h', 'corg']},\n",
              " 'cluster4': {'all': ['ds',\n",
              "   'sand',\n",
              "   'silt',\n",
              "   'clay',\n",
              "   'phh2o',\n",
              "   'sb',\n",
              "   'al',\n",
              "   'h',\n",
              "   'corg'],\n",
              "  'corrleq20': ['ds', 'sand', 'al', 'h', 'corg']}}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Estava dando erro: 'path_regioes' e não consegui descobrir o que é isso. Abri o arquivo 'dataset' e tem uma coluna chamada 'regiao' (coluna 12). Usei essa coluna para o pub_reg. O problema é que existe essa coluna 'regiao' nas 3 abas (o arquivo 'dataset' tem 3 abas: 'training', 'validation' e 'test') e não sei se pegou nas 3 abas ou somente da primeira.**"
      ],
      "metadata": {
        "id": "01PBsktlwlml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U9AjvnzcVCz"
      },
      "outputs": [],
      "source": [
        "dfs_training = pd.read_excel(f'{caminho_}/data/Treinamento-validacao-solos-minerais.xlsx', \n",
        "                    sheet_name='treinamento')\n",
        "\n",
        "dfs_test = pd.read_excel(f'{caminho_}/data/Treinamento-validacao-solos-minerais.xlsx', \n",
        "                    sheet_name='validacao')\n",
        "\n",
        "pub_reg = pd.read_excel(f'{caminho_}/data/pub.xls')\n",
        "\n",
        "#pub_reg = pd.read_excel(settings['dataset']['path'],\n",
        "                        #usecols=[12])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Está dando erro no 'all'. O Roberto me falou que o 'all' são todas as variáveis do 'dataset' mas não sei como corrigir esse erro.**"
      ],
      "metadata": {
        "id": "sM2mSYlUyQv5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRAKOjqfcVCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82c70a0d-5d53-41b1-c2c6-ed4683a99f52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator PCA from version 0.24.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "datasetColumns = add_pca(datasetColumns)\n",
        "datasetColumns = add_regioes(datasetColumns)\n",
        "reg = pd.read_json('https://servicodados.ibge.gov.br/api/v1/localidades/regioes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXLg4nGWcVC0"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    \"max_depth\": list(range(1, 150, 2)),\n",
        "    \"n_estimators\": list(range(1, 100, 2)),\n",
        "    #\"max_features\": ['auto', 'sqrt', 'log2'],\n",
        "    #\"bootstrap\": [True, False]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Não precisou usar o 'test_size' pois no 'dataset' já estão separados os treinos, testes e validação. Roberto disse que é mais fácil de produzir. Eu, particularmente, prefiro usar o 'test_size' por ser menos trabalhoso.**"
      ],
      "metadata": {
        "id": "Iv2dev9Ry-lB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**y = ds (densidade do solo)**\n"
      ],
      "metadata": {
        "id": "mQVhhg8_ziy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**x = restante das variáveis**"
      ],
      "metadata": {
        "id": "AkZK7f7nzqdn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0fqZWJkcVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5bd153-1942-41af-ff56-e845ca09153e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "iter: 1\n",
            "n_candidates: 1250\n",
            "n_resources: 30\n",
            "Fitting 5 folds for each of 1250 candidates, totalling 6250 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 417\n",
            "n_resources: 90\n",
            "Fitting 5 folds for each of 417 candidates, totalling 2085 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 139\n",
            "n_resources: 270\n",
            "Fitting 5 folds for each of 139 candidates, totalling 695 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator KMeans from version 0.24.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator PCA from version 0.24.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 4\n",
            "n_required_iterations: 8\n",
            "n_possible_iterations: 4\n",
            "min_resources_: 10\n",
            "max_resources_: 340\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 3750\n",
            "n_resources: 10\n",
            "Fitting 5 folds for each of 3750 candidates, totalling 18750 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 1250\n",
            "n_resources: 30\n",
            "Fitting 5 folds for each of 1250 candidates, totalling 6250 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 417\n",
            "n_resources: 90\n",
            "Fitting 5 folds for each of 417 candidates, totalling 2085 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 139\n",
            "n_resources: 270\n",
            "Fitting 5 folds for each of 139 candidates, totalling 695 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator KMeans from version 0.24.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator PCA from version 0.24.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 5\n",
            "n_required_iterations: 8\n",
            "n_possible_iterations: 5\n",
            "min_resources_: 10\n",
            "max_resources_: 1124\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 3750\n",
            "n_resources: 10\n",
            "Fitting 5 folds for each of 3750 candidates, totalling 18750 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 1250\n",
            "n_resources: 30\n",
            "Fitting 5 folds for each of 1250 candidates, totalling 6250 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 417\n",
            "n_resources: 90\n",
            "Fitting 5 folds for each of 417 candidates, totalling 2085 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 139\n",
            "n_resources: 270\n",
            "Fitting 5 folds for each of 139 candidates, totalling 695 fits\n",
            "----------\n",
            "iter: 4\n",
            "n_candidates: 47\n",
            "n_resources: 810\n",
            "Fitting 5 folds for each of 47 candidates, totalling 235 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator KMeans from version 0.24.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator PCA from version 0.24.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 3\n",
            "n_required_iterations: 8\n",
            "n_possible_iterations: 3\n",
            "min_resources_: 10\n",
            "max_resources_: 176\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 3750\n",
            "n_resources: 10\n",
            "Fitting 5 folds for each of 3750 candidates, totalling 18750 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 1250\n",
            "n_resources: 30\n",
            "Fitting 5 folds for each of 1250 candidates, totalling 6250 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 417\n",
            "n_resources: 90\n",
            "Fitting 5 folds for each of 417 candidates, totalling 2085 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator KMeans from version 0.24.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator PCA from version 0.24.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 3\n",
            "n_required_iterations: 8\n",
            "n_possible_iterations: 3\n",
            "min_resources_: 10\n",
            "max_resources_: 201\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 3750\n",
            "n_resources: 10\n",
            "Fitting 5 folds for each of 3750 candidates, totalling 18750 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 1250\n",
            "n_resources: 30\n",
            "Fitting 5 folds for each of 1250 candidates, totalling 6250 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 417\n",
            "n_resources: 90\n",
            "Fitting 5 folds for each of 417 candidates, totalling 2085 fits\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "for kcluster in datasetColumns:\n",
        "    for key in datasetColumns[kcluster]:\n",
        "\n",
        "        if kcluster not in reg.nome.to_list():\n",
        "            continue\n",
        "\n",
        "        dfs_training_x, dfs_training_y, dfs_test_x, dfs_test_y = preprare_training(datasetColumns, kcluster, key, dfs_training, dfs_test, pub_reg)\n",
        "\n",
        "        ini = time.time()\n",
        "        sh = HalvingGridSearchCV(\n",
        "            RandomForestRegressor(criterion=\"squared_error\"),\n",
        "            param_grid,\n",
        "            cv=5,\n",
        "            n_jobs=6,\n",
        "            scoring='neg_mean_squared_error',\n",
        "            verbose=2\n",
        "\n",
        "        ).fit(\n",
        "            X=dfs_training_x, \n",
        "            y=dfs_training_y\n",
        "            )\n",
        "        t = time.time() - ini\n",
        "\n",
        "        predict = sh.best_estimator_.predict(dfs_test_x)\n",
        "\n",
        "        #pickle.dump(sh.best_estimator_, open(f'{caminho_}/models/rf/' + kcluster + '_' + key + '.sav', 'wb'))\n",
        "        pickle.dump(sh.best_estimator_, open(f'{caminho_}/models/rf/' + kcluster + '_' + key + '.sav', 'wb'))\n",
        "\n",
        "        line = ['RF', kcluster, key, t, mean_squared_error(dfs_test_y, predict), mean_absolute_error(dfs_test_y, predict), mean_absolute_percentage_error(dfs_test_y, predict), str(sh.best_estimator_)]\n",
        "\n",
        "        results.append(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**'time': tempo de execução em segundos.**"
      ],
      "metadata": {
        "id": "7Stn_wV50DfJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3Wt5FhMcVC2"
      },
      "outputs": [],
      "source": [
        "dfr = pd.DataFrame(results, columns=['Method', 'Cluster', 'DatasetColumns', 'time', 'mse', 'mae', 'mape', 'coef'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Provavelmente terei que mudar o path do rf2.pkl.**"
      ],
      "metadata": {
        "id": "Qrzg_7fN3YID"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob1RscW9cVC2"
      },
      "outputs": [],
      "source": [
        "dfr.to_pickle(f'{caminho_}/results/rf2.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado**"
      ],
      "metadata": {
        "id": "RWvUvDHy3nG1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZhWo5FzcVC2",
        "outputId": "7a37dd0b-aa8f-4c44-e51a-f918b029a51e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Method       Cluster DatasetColumns         time       mse       mae  \\\n",
            "0     RF         Norte            all  2399.818646  0.013278  0.089942   \n",
            "1     RF      Nordeste            all  2482.483153  0.027928  0.132922   \n",
            "2     RF       Sudeste            all  2321.241932  0.015572  0.100720   \n",
            "3     RF           Sul            all  2146.935604  0.013886  0.092801   \n",
            "4     RF  Centro-Oeste            all  2199.935708  0.007049  0.068339   \n",
            "\n",
            "       mape                                               coef  \n",
            "0  0.072975  RandomForestRegressor(max_depth=23, n_estimato...  \n",
            "1  0.092523  RandomForestRegressor(max_depth=3, n_estimator...  \n",
            "2  0.079634  RandomForestRegressor(max_depth=77, n_estimato...  \n",
            "3  0.083011  RandomForestRegressor(max_depth=131, n_estimat...  \n",
            "4  0.063722  RandomForestRegressor(max_depth=91, n_estimato...  \n"
          ]
        }
      ],
      "source": [
        "print(dfr)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}